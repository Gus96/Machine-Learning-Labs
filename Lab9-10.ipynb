{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "mnist_trainset = datasets.MNIST(root='./data', train = True, download = True, transform = transform)\n",
    "mnist_testset = datasets.MNIST(root='./data', train = False, download = True, transform = transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_trainset[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(mnist_trainset, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(mnist_testset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# определяем гиперпараметры сети и процесса обучения\n",
    "inputs, n_hidden0, n_hidden1, out = 784, 128, 64, 10 \n",
    "n_epochs = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# определяем нейросеть\n",
    "model = nn.Sequential(nn.Linear(inputs, n_hidden0, bias=True),nn.ReLU(),\n",
    "                      nn.Linear(n_hidden0, n_hidden1, bias=True),nn.ReLU(),\n",
    "                      nn.Linear(n_hidden1, out, bias=True)\n",
    ") \n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# определяем функцию ошибки\n",
    "criterion = nn.CrossEntropyLoss() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# определяем оптимизатор - реализацию численного алгоритма оптимизации\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.5)\n",
    "vec_loss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Training loss: 0.3771170194405737\n",
      "Epoch 1 - Training loss: 0.13384375396643333\n",
      "Epoch 2 - Training loss: 0.093007630994742\n",
      "Epoch 3 - Training loss: 0.0711449901439321\n",
      "Epoch 4 - Training loss: 0.05829745617611751\n",
      "Epoch 5 - Training loss: 0.04630432663701856\n",
      "Epoch 6 - Training loss: 0.0381225755286546\n",
      "Epoch 7 - Training loss: 0.030728414479239997\n",
      "Epoch 8 - Training loss: 0.025965377149335455\n",
      "Epoch 9 - Training loss: 0.021606438867894234\n",
      "Epoch 10 - Training loss: 0.01769160694386964\n",
      "Epoch 11 - Training loss: 0.014223795064042003\n",
      "Epoch 12 - Training loss: 0.01003774450242972\n",
      "Epoch 13 - Training loss: 0.00786795428003451\n",
      "Epoch 14 - Training loss: 0.006218939734697178\n",
      "Epoch 15 - Training loss: 0.004051118692621803\n",
      "Epoch 16 - Training loss: 0.00265518261138782\n",
      "Epoch 17 - Training loss: 0.0017308918593016158\n",
      "Epoch 18 - Training loss: 0.00114580494169662\n",
      "Epoch 19 - Training loss: 0.0008993410635571163\n",
      "Epoch 20 - Training loss: 0.0007657991762569699\n",
      "Epoch 21 - Training loss: 0.0006565844482327768\n",
      "Epoch 22 - Training loss: 0.0005712572234092952\n",
      "Epoch 23 - Training loss: 0.0005189110082873023\n",
      "Epoch 24 - Training loss: 0.0004803814129528914\n",
      "Epoch 25 - Training loss: 0.00044252116197064184\n",
      "Epoch 26 - Training loss: 0.00041504334338650394\n",
      "Epoch 27 - Training loss: 0.0003877888190721611\n",
      "Epoch 28 - Training loss: 0.0003602982223019896\n",
      "Epoch 29 - Training loss: 0.000343105181416459\n",
      "Epoch 30 - Training loss: 0.0003211820483742144\n",
      "Epoch 31 - Training loss: 0.00030600420258117744\n",
      "Epoch 32 - Training loss: 0.0002929464709463008\n",
      "Epoch 33 - Training loss: 0.00027776401430576665\n",
      "Epoch 34 - Training loss: 0.0002652957292641682\n",
      "Epoch 35 - Training loss: 0.00025394917703099794\n",
      "Epoch 36 - Training loss: 0.00024330711861546617\n",
      "Epoch 37 - Training loss: 0.00023364843199776895\n",
      "Epoch 38 - Training loss: 0.00022364003221204702\n",
      "Epoch 39 - Training loss: 0.00021629203993681298\n",
      "Epoch 40 - Training loss: 0.0002084195767843967\n",
      "Epoch 41 - Training loss: 0.00020089509697477625\n",
      "Epoch 42 - Training loss: 0.00019392658838315294\n",
      "Epoch 43 - Training loss: 0.00018837920253663703\n",
      "Epoch 44 - Training loss: 0.0001824689273022636\n",
      "Epoch 45 - Training loss: 0.0001761477666226185\n",
      "Epoch 46 - Training loss: 0.00017138108256158628\n",
      "Epoch 47 - Training loss: 0.0001662861183326849\n",
      "Epoch 48 - Training loss: 0.00016115454468020843\n",
      "Epoch 49 - Training loss: 0.0001574054489348156\n",
      "Epoch 50 - Training loss: 0.00015235700399051678\n",
      "Epoch 51 - Training loss: 0.00014835370177164367\n",
      "Epoch 52 - Training loss: 0.0001448248843906773\n",
      "Epoch 53 - Training loss: 0.00014095828945996248\n",
      "Epoch 54 - Training loss: 0.00013757551751007383\n",
      "Epoch 55 - Training loss: 0.0001340282807161938\n",
      "Epoch 56 - Training loss: 0.0001307943762506037\n",
      "Epoch 57 - Training loss: 0.00012768632860339738\n",
      "Epoch 58 - Training loss: 0.00012441069194120717\n",
      "Epoch 59 - Training loss: 0.00012203087173333506\n",
      "Epoch 60 - Training loss: 0.0001189987030865043\n",
      "Epoch 61 - Training loss: 0.00011698102179740315\n",
      "Epoch 62 - Training loss: 0.0001140827818149727\n",
      "Epoch 63 - Training loss: 0.00011156538433462837\n",
      "Epoch 64 - Training loss: 0.00010980560948056328\n",
      "Epoch 65 - Training loss: 0.00010745750708833635\n",
      "Epoch 66 - Training loss: 0.00010529929755554671\n",
      "Epoch 67 - Training loss: 0.00010303323371977494\n",
      "Epoch 68 - Training loss: 0.00010107347827982403\n",
      "Epoch 69 - Training loss: 9.930331009115284e-05\n",
      "Epoch 70 - Training loss: 9.744689345573462e-05\n",
      "Epoch 71 - Training loss: 9.556282396303118e-05\n",
      "Epoch 72 - Training loss: 9.38466770140415e-05\n",
      "Epoch 73 - Training loss: 9.228441257161469e-05\n",
      "Epoch 74 - Training loss: 9.065048426139453e-05\n",
      "Epoch 75 - Training loss: 8.911577038749031e-05\n",
      "Epoch 76 - Training loss: 8.753918621605136e-05\n",
      "Epoch 77 - Training loss: 8.633473112543612e-05\n",
      "Epoch 78 - Training loss: 8.493625080415277e-05\n",
      "Epoch 79 - Training loss: 8.339579324862064e-05\n",
      "Epoch 80 - Training loss: 8.188274378436952e-05\n",
      "Epoch 81 - Training loss: 8.079931617235568e-05\n",
      "Epoch 82 - Training loss: 7.958045436958646e-05\n",
      "Epoch 83 - Training loss: 7.834413639190657e-05\n",
      "Epoch 84 - Training loss: 7.728926824813674e-05\n",
      "Epoch 85 - Training loss: 7.567002866386123e-05\n",
      "Epoch 86 - Training loss: 7.483657582473716e-05\n",
      "Epoch 87 - Training loss: 7.370426015692687e-05\n",
      "Epoch 88 - Training loss: 7.260926998572243e-05\n",
      "Epoch 89 - Training loss: 7.179745708373554e-05\n",
      "Epoch 90 - Training loss: 7.057961168564009e-05\n",
      "Epoch 91 - Training loss: 6.958541229599213e-05\n",
      "Epoch 92 - Training loss: 6.866479062332938e-05\n",
      "Epoch 93 - Training loss: 6.783242505512396e-05\n",
      "Epoch 94 - Training loss: 6.686069823646796e-05\n",
      "Epoch 95 - Training loss: 6.595375916487206e-05\n",
      "Epoch 96 - Training loss: 6.515882493729404e-05\n",
      "Epoch 97 - Training loss: 6.421842611314599e-05\n",
      "Epoch 98 - Training loss: 6.35000796033268e-05\n",
      "Epoch 99 - Training loss: 6.261196647556532e-05\n",
      "Epoch 100 - Training loss: 6.181787728982716e-05\n",
      "Epoch 101 - Training loss: 6.115797441555763e-05\n",
      "Epoch 102 - Training loss: 6.038448140240305e-05\n",
      "Epoch 103 - Training loss: 5.964513642585346e-05\n",
      "Epoch 104 - Training loss: 5.886924526106283e-05\n",
      "Epoch 105 - Training loss: 5.815370313137286e-05\n",
      "Epoch 106 - Training loss: 5.756533225943662e-05\n",
      "Epoch 107 - Training loss: 5.681578695741292e-05\n",
      "Epoch 108 - Training loss: 5.615273450117313e-05\n",
      "Epoch 109 - Training loss: 5.566279193097818e-05\n",
      "Epoch 110 - Training loss: 5.4938760783117495e-05\n",
      "Epoch 111 - Training loss: 5.433212901432219e-05\n",
      "Epoch 112 - Training loss: 5.368968017445713e-05\n",
      "Epoch 113 - Training loss: 5.304347088000327e-05\n",
      "Epoch 114 - Training loss: 5.2518944939758616e-05\n",
      "Epoch 115 - Training loss: 5.18798057778079e-05\n",
      "Epoch 116 - Training loss: 5.1364288920203054e-05\n",
      "Epoch 117 - Training loss: 5.0848947271559036e-05\n",
      "Epoch 118 - Training loss: 5.025997431643305e-05\n",
      "Epoch 119 - Training loss: 4.982230401783626e-05\n",
      "Epoch 120 - Training loss: 4.929863647174149e-05\n",
      "Epoch 121 - Training loss: 4.876654707883806e-05\n",
      "Epoch 122 - Training loss: 4.8258295971029875e-05\n",
      "Epoch 123 - Training loss: 4.768994604985112e-05\n",
      "Epoch 124 - Training loss: 4.735229682257543e-05\n",
      "Epoch 125 - Training loss: 4.67835332859558e-05\n",
      "Epoch 126 - Training loss: 4.637070471440449e-05\n",
      "Epoch 127 - Training loss: 4.59168697978669e-05\n",
      "Epoch 128 - Training loss: 4.548196860258871e-05\n",
      "Epoch 129 - Training loss: 4.5064552531398755e-05\n",
      "Epoch 130 - Training loss: 4.459746380376161e-05\n",
      "Epoch 131 - Training loss: 4.426969582306359e-05\n",
      "Epoch 132 - Training loss: 4.373573090588601e-05\n",
      "Epoch 133 - Training loss: 4.3431338579415235e-05\n",
      "Epoch 134 - Training loss: 4.30142096861845e-05\n",
      "Epoch 135 - Training loss: 4.2567512320596144e-05\n",
      "Epoch 136 - Training loss: 4.233934235839215e-05\n",
      "Epoch 137 - Training loss: 4.185281399284897e-05\n",
      "Epoch 138 - Training loss: 4.151221382096828e-05\n",
      "Epoch 139 - Training loss: 4.115389279281883e-05\n",
      "Epoch 140 - Training loss: 4.0690458664416626e-05\n",
      "Epoch 141 - Training loss: 4.040503960676402e-05\n",
      "Epoch 142 - Training loss: 4.00232711589953e-05\n",
      "Epoch 143 - Training loss: 3.966770489048066e-05\n",
      "Epoch 144 - Training loss: 3.930147248860403e-05\n",
      "Epoch 145 - Training loss: 3.910181352096409e-05\n",
      "Epoch 146 - Training loss: 3.8676110692873e-05\n",
      "Epoch 147 - Training loss: 3.837848644090865e-05\n",
      "Epoch 148 - Training loss: 3.802249391638271e-05\n",
      "Epoch 149 - Training loss: 3.781537992958688e-05\n",
      "Epoch 150 - Training loss: 3.7440497633361954e-05\n",
      "Epoch 151 - Training loss: 3.711920946920133e-05\n",
      "Epoch 152 - Training loss: 3.681026165358e-05\n",
      "Epoch 153 - Training loss: 3.655739793503705e-05\n",
      "Epoch 154 - Training loss: 3.62530116522322e-05\n",
      "Epoch 155 - Training loss: 3.596176488164145e-05\n",
      "Epoch 156 - Training loss: 3.566575992231359e-05\n",
      "Epoch 157 - Training loss: 3.540099449816901e-05\n",
      "Epoch 158 - Training loss: 3.510707255968722e-05\n",
      "Epoch 159 - Training loss: 3.483587245236166e-05\n",
      "Epoch 160 - Training loss: 3.462047638491459e-05\n",
      "Epoch 161 - Training loss: 3.432910305859136e-05\n",
      "Epoch 162 - Training loss: 3.4071788203961355e-05\n",
      "Epoch 163 - Training loss: 3.3825887530465376e-05\n",
      "Epoch 164 - Training loss: 3.3604048227569215e-05\n",
      "Epoch 165 - Training loss: 3.3317391239728026e-05\n",
      "Epoch 166 - Training loss: 3.3058146501403335e-05\n",
      "Epoch 167 - Training loss: 3.282641527891859e-05\n",
      "Epoch 168 - Training loss: 3.2645145886207584e-05\n",
      "Epoch 169 - Training loss: 3.2343613858753636e-05\n",
      "Epoch 170 - Training loss: 3.2111483440130055e-05\n",
      "Epoch 171 - Training loss: 3.186847830073418e-05\n",
      "Epoch 172 - Training loss: 3.168230128265715e-05\n",
      "Epoch 173 - Training loss: 3.1432282418333247e-05\n",
      "Epoch 174 - Training loss: 3.121911400811851e-05\n",
      "Epoch 175 - Training loss: 3.0978518338270465e-05\n",
      "Epoch 176 - Training loss: 3.078715893777637e-05\n",
      "Epoch 177 - Training loss: 3.058441796125e-05\n",
      "Epoch 178 - Training loss: 3.036091548736461e-05\n",
      "Epoch 179 - Training loss: 3.0168192722749082e-05\n",
      "Epoch 180 - Training loss: 3.0007489948422256e-05\n",
      "Epoch 181 - Training loss: 2.9753628897922415e-05\n",
      "Epoch 182 - Training loss: 2.9576356881389877e-05\n",
      "Epoch 183 - Training loss: 2.93767684656895e-05\n",
      "Epoch 184 - Training loss: 2.9231478205657112e-05\n",
      "Epoch 185 - Training loss: 2.896784659901108e-05\n",
      "Epoch 186 - Training loss: 2.8788585989720655e-05\n",
      "Epoch 187 - Training loss: 2.8609160768691633e-05\n",
      "Epoch 188 - Training loss: 2.841376896007868e-05\n",
      "Epoch 189 - Training loss: 2.8233050653528507e-05\n",
      "Epoch 190 - Training loss: 2.8053842457400262e-05\n",
      "Epoch 191 - Training loss: 2.7877183770415027e-05\n",
      "Epoch 192 - Training loss: 2.7715918411397217e-05\n",
      "Epoch 193 - Training loss: 2.7527510282634265e-05\n",
      "Epoch 194 - Training loss: 2.7353633306094053e-05\n",
      "Epoch 195 - Training loss: 2.7211177585026736e-05\n",
      "Epoch 196 - Training loss: 2.7022624217120636e-05\n",
      "Epoch 197 - Training loss: 2.6853784454431387e-05\n",
      "Epoch 198 - Training loss: 2.669152983693792e-05\n",
      "Epoch 199 - Training loss: 2.654600549721505e-05\n",
      "Epoch 200 - Training loss: 2.6423463975070645e-05\n",
      "Epoch 201 - Training loss: 2.619833630226265e-05\n",
      "Epoch 202 - Training loss: 2.6067741833197718e-05\n",
      "Epoch 203 - Training loss: 2.588213691164199e-05\n",
      "Epoch 204 - Training loss: 2.576136588892719e-05\n",
      "Epoch 205 - Training loss: 2.557939786086619e-05\n",
      "Epoch 206 - Training loss: 2.543204838112505e-05\n",
      "Epoch 207 - Training loss: 2.5289879990825297e-05\n",
      "Epoch 208 - Training loss: 2.514826765748769e-05\n",
      "Epoch 209 - Training loss: 2.501193732048483e-05\n",
      "Epoch 210 - Training loss: 2.484131458304977e-05\n",
      "Epoch 211 - Training loss: 2.474296185548605e-05\n",
      "Epoch 212 - Training loss: 2.4574109479216402e-05\n",
      "Epoch 213 - Training loss: 2.4435588283959194e-05\n",
      "Epoch 214 - Training loss: 2.4303000003987788e-05\n",
      "Epoch 215 - Training loss: 2.4138398717421328e-05\n",
      "Epoch 216 - Training loss: 2.4023920354757226e-05\n",
      "Epoch 217 - Training loss: 2.3900316448877703e-05\n",
      "Epoch 218 - Training loss: 2.3776652187028728e-05\n",
      "Epoch 219 - Training loss: 2.3614970898166454e-05\n",
      "Epoch 220 - Training loss: 2.352389475304381e-05\n",
      "Epoch 221 - Training loss: 2.3376945704687474e-05\n",
      "Epoch 222 - Training loss: 2.3246107568300734e-05\n",
      "Epoch 223 - Training loss: 2.311652031651245e-05\n",
      "Epoch 224 - Training loss: 2.299355153196917e-05\n",
      "Epoch 225 - Training loss: 2.290304626228158e-05\n",
      "Epoch 226 - Training loss: 2.2748416465150035e-05\n",
      "Epoch 227 - Training loss: 2.262876217139898e-05\n",
      "Epoch 228 - Training loss: 2.2515811513128333e-05\n",
      "Epoch 229 - Training loss: 2.23924299837484e-05\n",
      "Epoch 230 - Training loss: 2.2278177055977744e-05\n",
      "Epoch 231 - Training loss: 2.2167768438602815e-05\n",
      "Epoch 232 - Training loss: 2.2039433750702727e-05\n",
      "Epoch 233 - Training loss: 2.192708764216716e-05\n",
      "Epoch 234 - Training loss: 2.1810226164632726e-05\n",
      "Epoch 235 - Training loss: 2.1703258382399835e-05\n",
      "Epoch 236 - Training loss: 2.158955488316691e-05\n",
      "Epoch 237 - Training loss: 2.1479326461668207e-05\n",
      "Epoch 238 - Training loss: 2.1370145009380663e-05\n",
      "Epoch 239 - Training loss: 2.126358981207443e-05\n",
      "Epoch 240 - Training loss: 2.115137670997297e-05\n",
      "Epoch 241 - Training loss: 2.1064599063684625e-05\n",
      "Epoch 242 - Training loss: 2.095084674116277e-05\n",
      "Epoch 243 - Training loss: 2.083538934034298e-05\n",
      "Epoch 244 - Training loss: 2.0756304917776767e-05\n",
      "Epoch 245 - Training loss: 2.0638597005200875e-05\n",
      "Epoch 246 - Training loss: 2.0553171096776556e-05\n",
      "Epoch 247 - Training loss: 2.046205849372508e-05\n",
      "Epoch 248 - Training loss: 2.0349634516542184e-05\n",
      "Epoch 249 - Training loss: 2.024854468217571e-05\n",
      "Epoch 250 - Training loss: 2.013816752296308e-05\n",
      "Epoch 251 - Training loss: 2.0050614710156698e-05\n",
      "Epoch 252 - Training loss: 1.9964355980826086e-05\n",
      "Epoch 253 - Training loss: 1.987897887484786e-05\n",
      "Epoch 254 - Training loss: 1.976392664813035e-05\n",
      "Epoch 255 - Training loss: 1.9671777324461914e-05\n",
      "Epoch 256 - Training loss: 1.9576298514053625e-05\n",
      "Epoch 257 - Training loss: 1.9500093422489894e-05\n",
      "Epoch 258 - Training loss: 1.941160469531737e-05\n",
      "Epoch 259 - Training loss: 1.9316332563534965e-05\n",
      "Epoch 260 - Training loss: 1.9224154511457158e-05\n",
      "Epoch 261 - Training loss: 1.913292110185021e-05\n",
      "Epoch 262 - Training loss: 1.9042366892560192e-05\n",
      "Epoch 263 - Training loss: 1.896208293675389e-05\n",
      "Epoch 264 - Training loss: 1.8879869092959247e-05\n",
      "Epoch 265 - Training loss: 1.8780193720510658e-05\n",
      "Epoch 266 - Training loss: 1.8696558487331477e-05\n",
      "Epoch 267 - Training loss: 1.864180092138631e-05\n",
      "Epoch 268 - Training loss: 1.853046775053425e-05\n",
      "Epoch 269 - Training loss: 1.8453856267366898e-05\n",
      "Epoch 270 - Training loss: 1.8384864396281328e-05\n",
      "Epoch 271 - Training loss: 1.8342003048911656e-05\n",
      "Epoch 272 - Training loss: 1.8202117545183466e-05\n",
      "Epoch 273 - Training loss: 1.8152401612365863e-05\n",
      "Epoch 274 - Training loss: 1.805151957322273e-05\n",
      "Epoch 275 - Training loss: 1.7970859245194956e-05\n",
      "Epoch 276 - Training loss: 1.791589313894134e-05\n",
      "Epoch 277 - Training loss: 1.7811897972213537e-05\n",
      "Epoch 278 - Training loss: 1.7743567779220523e-05\n",
      "Epoch 279 - Training loss: 1.7660403664213533e-05\n",
      "Epoch 280 - Training loss: 1.7587862085961516e-05\n",
      "Epoch 281 - Training loss: 1.7504171939714162e-05\n",
      "Epoch 282 - Training loss: 1.7452104857026174e-05\n",
      "Epoch 283 - Training loss: 1.7364036215786045e-05\n",
      "Epoch 284 - Training loss: 1.7295298394952996e-05\n",
      "Epoch 285 - Training loss: 1.7220293011438827e-05\n",
      "Epoch 286 - Training loss: 1.7140802845881646e-05\n",
      "Epoch 287 - Training loss: 1.7072921455381075e-05\n",
      "Epoch 288 - Training loss: 1.7005335933542462e-05\n",
      "Epoch 289 - Training loss: 1.6927426109869644e-05\n",
      "Epoch 290 - Training loss: 1.6855759588445383e-05\n",
      "Epoch 291 - Training loss: 1.680344216845048e-05\n",
      "Epoch 292 - Training loss: 1.6744442105395954e-05\n",
      "Epoch 293 - Training loss: 1.6658233514199214e-05\n",
      "Epoch 294 - Training loss: 1.659629939940538e-05\n",
      "Epoch 295 - Training loss: 1.6535656705979872e-05\n",
      "Epoch 296 - Training loss: 1.6454223979043464e-05\n",
      "Epoch 297 - Training loss: 1.6392466570512614e-05\n",
      "Epoch 298 - Training loss: 1.6320310509315505e-05\n",
      "Epoch 299 - Training loss: 1.6270893952468493e-05\n",
      "Epoch 300 - Training loss: 1.6188684500501427e-05\n",
      "Epoch 301 - Training loss: 1.6122738529982448e-05\n",
      "Epoch 302 - Training loss: 1.608195559374089e-05\n",
      "Epoch 303 - Training loss: 1.6006599655866482e-05\n",
      "Epoch 304 - Training loss: 1.5950007957605974e-05\n",
      "Epoch 305 - Training loss: 1.5894642392831607e-05\n",
      "Epoch 306 - Training loss: 1.5827628372995313e-05\n",
      "Epoch 307 - Training loss: 1.5758656268917008e-05\n",
      "Epoch 308 - Training loss: 1.568417227876487e-05\n",
      "Epoch 309 - Training loss: 1.5635819102893645e-05\n",
      "Epoch 310 - Training loss: 1.55698856251022e-05\n",
      "Epoch 311 - Training loss: 1.5512830923564655e-05\n",
      "Epoch 312 - Training loss: 1.5455670530088614e-05\n",
      "Epoch 313 - Training loss: 1.539641735026655e-05\n",
      "Epoch 314 - Training loss: 1.53582847524924e-05\n",
      "Epoch 315 - Training loss: 1.5270756438703496e-05\n",
      "Epoch 316 - Training loss: 1.5218478994672377e-05\n",
      "Epoch 317 - Training loss: 1.5149564927213411e-05\n",
      "Epoch 318 - Training loss: 1.5131387888297433e-05\n",
      "Epoch 319 - Training loss: 1.5049695089994546e-05\n",
      "Epoch 320 - Training loss: 1.4992445137753927e-05\n",
      "Epoch 321 - Training loss: 1.4952837916749454e-05\n",
      "Epoch 322 - Training loss: 1.4880917348129215e-05\n",
      "Epoch 323 - Training loss: 1.4834893790740478e-05\n",
      "Epoch 324 - Training loss: 1.4770680619463662e-05\n",
      "Epoch 325 - Training loss: 1.4718529218337634e-05\n",
      "Epoch 326 - Training loss: 1.4652269907933461e-05\n",
      "Epoch 327 - Training loss: 1.460935146818227e-05\n",
      "Epoch 328 - Training loss: 1.4547800830738759e-05\n",
      "Epoch 329 - Training loss: 1.4510144473778865e-05\n",
      "Epoch 330 - Training loss: 1.4451833793883518e-05\n",
      "Epoch 331 - Training loss: 1.4396376272202482e-05\n",
      "Epoch 332 - Training loss: 1.4345337321052227e-05\n",
      "Epoch 333 - Training loss: 1.429747604219453e-05\n",
      "Epoch 334 - Training loss: 1.4236704105154871e-05\n",
      "Epoch 335 - Training loss: 1.419918475396205e-05\n",
      "Epoch 336 - Training loss: 1.4139905897254114e-05\n",
      "Epoch 337 - Training loss: 1.4095511377781073e-05\n",
      "Epoch 338 - Training loss: 1.4047414634518236e-05\n",
      "Epoch 339 - Training loss: 1.4000708881409068e-05\n",
      "Epoch 340 - Training loss: 1.3943528251233714e-05\n",
      "Epoch 341 - Training loss: 1.3906454045436916e-05\n",
      "Epoch 342 - Training loss: 1.384626218751017e-05\n",
      "Epoch 343 - Training loss: 1.3796309142979693e-05\n",
      "Epoch 344 - Training loss: 1.3759280618013254e-05\n",
      "Epoch 345 - Training loss: 1.3703092062566355e-05\n",
      "Epoch 346 - Training loss: 1.3649136563032585e-05\n",
      "Epoch 347 - Training loss: 1.3610119352936282e-05\n",
      "Epoch 348 - Training loss: 1.3561318515732748e-05\n",
      "Epoch 349 - Training loss: 1.3520564195073494e-05\n",
      "Epoch 350 - Training loss: 1.3469026961690897e-05\n",
      "Epoch 351 - Training loss: 1.3424146159507004e-05\n",
      "Epoch 352 - Training loss: 1.3372135688443191e-05\n",
      "Epoch 353 - Training loss: 1.3337186074485502e-05\n",
      "Epoch 354 - Training loss: 1.3284448346071164e-05\n",
      "Epoch 355 - Training loss: 1.3243583348731345e-05\n",
      "Epoch 356 - Training loss: 1.3215617102870194e-05\n",
      "Epoch 357 - Training loss: 1.3152184650037548e-05\n",
      "Epoch 358 - Training loss: 1.3122908357113266e-05\n",
      "Epoch 359 - Training loss: 1.3067711588954651e-05\n",
      "Epoch 360 - Training loss: 1.3028358706099391e-05\n",
      "Epoch 361 - Training loss: 1.2978216944767431e-05\n",
      "Epoch 362 - Training loss: 1.2938151959448941e-05\n",
      "Epoch 363 - Training loss: 1.2905667903276196e-05\n",
      "Epoch 364 - Training loss: 1.2856038025122581e-05\n",
      "Epoch 365 - Training loss: 1.2814183969547544e-05\n",
      "Epoch 366 - Training loss: 1.2772243154142197e-05\n",
      "Epoch 367 - Training loss: 1.274230001067973e-05\n",
      "Epoch 368 - Training loss: 1.2697333793301375e-05\n",
      "Epoch 369 - Training loss: 1.265243046070119e-05\n",
      "Epoch 370 - Training loss: 1.2607022902644545e-05\n",
      "Epoch 371 - Training loss: 1.2566437731987989e-05\n",
      "Epoch 372 - Training loss: 1.2544031967614081e-05\n",
      "Epoch 373 - Training loss: 1.2488082433972612e-05\n",
      "Epoch 374 - Training loss: 1.2443361667814372e-05\n",
      "Epoch 375 - Training loss: 1.2408278763390125e-05\n",
      "Epoch 376 - Training loss: 1.2368693417522274e-05\n",
      "Epoch 377 - Training loss: 1.2328618629107755e-05\n",
      "Epoch 378 - Training loss: 1.229624867952699e-05\n",
      "Epoch 379 - Training loss: 1.225205306282359e-05\n",
      "Epoch 380 - Training loss: 1.2214224106840728e-05\n",
      "Epoch 381 - Training loss: 1.217579057524252e-05\n",
      "Epoch 382 - Training loss: 1.2157398554584586e-05\n",
      "Epoch 383 - Training loss: 1.210232725576274e-05\n",
      "Epoch 384 - Training loss: 1.2060686936440278e-05\n",
      "Epoch 385 - Training loss: 1.2035668951637301e-05\n",
      "Epoch 386 - Training loss: 1.199587276107262e-05\n",
      "Epoch 387 - Training loss: 1.1950658222722803e-05\n",
      "Epoch 388 - Training loss: 1.1923304871161757e-05\n",
      "Epoch 389 - Training loss: 1.1884953879279195e-05\n",
      "Epoch 390 - Training loss: 1.1843796333219228e-05\n",
      "Epoch 391 - Training loss: 1.1813332369319049e-05\n",
      "Epoch 392 - Training loss: 1.1773355854376335e-05\n",
      "Epoch 393 - Training loss: 1.1739780429602793e-05\n",
      "Epoch 394 - Training loss: 1.1709551372170734e-05\n",
      "Epoch 395 - Training loss: 1.1666213274259625e-05\n",
      "Epoch 396 - Training loss: 1.1628610974542185e-05\n",
      "Epoch 397 - Training loss: 1.1597331163922281e-05\n",
      "Epoch 398 - Training loss: 1.1567705194749488e-05\n",
      "Epoch 399 - Training loss: 1.1544215173956743e-05\n",
      "Epoch 400 - Training loss: 1.1507651929501414e-05\n",
      "Epoch 401 - Training loss: 1.1458126843878767e-05\n",
      "Epoch 402 - Training loss: 1.1437905438937352e-05\n",
      "Epoch 403 - Training loss: 1.13923951097979e-05\n",
      "Epoch 404 - Training loss: 1.1367239360541678e-05\n",
      "Epoch 405 - Training loss: 1.1325365084816046e-05\n",
      "Epoch 406 - Training loss: 1.1298861420385416e-05\n",
      "Epoch 407 - Training loss: 1.1256377377592336e-05\n",
      "Epoch 408 - Training loss: 1.1235951559805587e-05\n",
      "Epoch 409 - Training loss: 1.1192114338672694e-05\n",
      "Epoch 410 - Training loss: 1.1161857071502843e-05\n",
      "Epoch 411 - Training loss: 1.112773419375793e-05\n",
      "Epoch 412 - Training loss: 1.1096061952564322e-05\n",
      "Epoch 413 - Training loss: 1.1065194531448624e-05\n",
      "Epoch 414 - Training loss: 1.1029033639081449e-05\n",
      "Epoch 415 - Training loss: 1.1020075919864035e-05\n",
      "Epoch 416 - Training loss: 1.0971614578151212e-05\n",
      "Epoch 417 - Training loss: 1.0935691520859055e-05\n",
      "Epoch 418 - Training loss: 1.0905583563166567e-05\n",
      "Epoch 419 - Training loss: 1.088336513529765e-05\n",
      "Epoch 420 - Training loss: 1.085149081631244e-05\n",
      "Epoch 421 - Training loss: 1.0816670207862284e-05\n",
      "Epoch 422 - Training loss: 1.0780336557719243e-05\n",
      "Epoch 423 - Training loss: 1.0761085032260407e-05\n",
      "Epoch 424 - Training loss: 1.0732322302142835e-05\n",
      "Epoch 425 - Training loss: 1.0703028159667883e-05\n",
      "Epoch 426 - Training loss: 1.0668212282841491e-05\n",
      "Epoch 427 - Training loss: 1.063470431690438e-05\n",
      "Epoch 428 - Training loss: 1.060776727163531e-05\n",
      "Epoch 429 - Training loss: 1.058156245815388e-05\n",
      "Epoch 430 - Training loss: 1.0549195830494863e-05\n",
      "Epoch 431 - Training loss: 1.0521397938033164e-05\n",
      "Epoch 432 - Training loss: 1.0490070415786142e-05\n",
      "Epoch 433 - Training loss: 1.0464880249302737e-05\n",
      "Epoch 434 - Training loss: 1.0429077576829492e-05\n",
      "Epoch 435 - Training loss: 1.0404364595220975e-05\n",
      "Epoch 436 - Training loss: 1.0378487110423486e-05\n",
      "Epoch 437 - Training loss: 1.034714931531738e-05\n",
      "Epoch 438 - Training loss: 1.0320906416394966e-05\n",
      "Epoch 439 - Training loss: 1.0300213903025519e-05\n",
      "Epoch 440 - Training loss: 1.0278367838647746e-05\n",
      "Epoch 441 - Training loss: 1.0247672177821052e-05\n",
      "Epoch 442 - Training loss: 1.0208122601368114e-05\n",
      "Epoch 443 - Training loss: 1.018284278634736e-05\n",
      "Epoch 444 - Training loss: 1.0153595644231e-05\n",
      "Epoch 445 - Training loss: 1.0128418478771687e-05\n",
      "Epoch 446 - Training loss: 1.0104484554027677e-05\n",
      "Epoch 447 - Training loss: 1.008181683117801e-05\n",
      "Epoch 448 - Training loss: 1.004712293170142e-05\n",
      "Epoch 449 - Training loss: 1.0029054967750918e-05\n",
      "Epoch 450 - Training loss: 9.994397755111858e-06\n",
      "Epoch 451 - Training loss: 9.969084983964292e-06\n",
      "Epoch 452 - Training loss: 9.941792417859993e-06\n",
      "Epoch 453 - Training loss: 9.91781692272422e-06\n",
      "Epoch 454 - Training loss: 9.891678091638836e-06\n",
      "Epoch 455 - Training loss: 9.8673169502627e-06\n",
      "Epoch 456 - Training loss: 9.841464087361774e-06\n",
      "Epoch 457 - Training loss: 9.819406283053743e-06\n",
      "Epoch 458 - Training loss: 9.788057390941356e-06\n",
      "Epoch 459 - Training loss: 9.761331590595758e-06\n",
      "Epoch 460 - Training loss: 9.746359655751053e-06\n",
      "Epoch 461 - Training loss: 9.71621588799434e-06\n",
      "Epoch 462 - Training loss: 9.688014285311264e-06\n",
      "Epoch 463 - Training loss: 9.664030183605402e-06\n",
      "Epoch 464 - Training loss: 9.635039587880849e-06\n",
      "Epoch 465 - Training loss: 9.626190663517632e-06\n",
      "Epoch 466 - Training loss: 9.592633004173112e-06\n",
      "Epoch 467 - Training loss: 9.564420341771153e-06\n",
      "Epoch 468 - Training loss: 9.5416467851443e-06\n",
      "Epoch 469 - Training loss: 9.518073752094929e-06\n",
      "Epoch 470 - Training loss: 9.491730069106277e-06\n",
      "Epoch 471 - Training loss: 9.468057402915556e-06\n",
      "Epoch 472 - Training loss: 9.447121143868357e-06\n",
      "Epoch 473 - Training loss: 9.426582845996189e-06\n",
      "Epoch 474 - Training loss: 9.40440407884453e-06\n",
      "Epoch 475 - Training loss: 9.371979737204086e-06\n",
      "Epoch 476 - Training loss: 9.351156968640896e-06\n",
      "Epoch 477 - Training loss: 9.333312817845209e-06\n",
      "Epoch 478 - Training loss: 9.313907247535582e-06\n",
      "Epoch 479 - Training loss: 9.285936183491145e-06\n",
      "Epoch 480 - Training loss: 9.261800657493559e-06\n",
      "Epoch 481 - Training loss: 9.255426651128434e-06\n",
      "Epoch 482 - Training loss: 9.214984038982716e-06\n",
      "Epoch 483 - Training loss: 9.198970575585909e-06\n",
      "Epoch 484 - Training loss: 9.176050148350513e-06\n",
      "Epoch 485 - Training loss: 9.155923031891828e-06\n",
      "Epoch 486 - Training loss: 9.13203981287694e-06\n",
      "Epoch 487 - Training loss: 9.103678554202994e-06\n",
      "Epoch 488 - Training loss: 9.07986101291812e-06\n",
      "Epoch 489 - Training loss: 9.057823926293049e-06\n",
      "Epoch 490 - Training loss: 9.042450926199574e-06\n",
      "Epoch 491 - Training loss: 9.018863195085121e-06\n",
      "Epoch 492 - Training loss: 9.002328970046495e-06\n",
      "Epoch 493 - Training loss: 8.973643839550984e-06\n",
      "Epoch 494 - Training loss: 8.954402644603292e-06\n",
      "Epoch 495 - Training loss: 8.938577865371553e-06\n",
      "Epoch 496 - Training loss: 8.906961335590801e-06\n",
      "Epoch 497 - Training loss: 8.889289503342132e-06\n",
      "Epoch 498 - Training loss: 8.869794443983308e-06\n",
      "Epoch 499 - Training loss: 8.843189936967035e-06\n"
     ]
    }
   ],
   "source": [
    "#обучаем модель\n",
    "for epoch in range(n_epochs): \n",
    "    running_loss = 0\n",
    "    for images, labels in trainloader:\n",
    "        \n",
    "        images = images.view(images.shape[0], -1)\n",
    "        y_pred = model(images) \n",
    "        loss = criterion(y_pred, labels)\n",
    "        optimizer.zero_grad() \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    else:\n",
    "        print(\"Epoch {} - Training loss: {}\".format(epoch, running_loss/len(trainloader)))\n",
    "        vec_loss.append(running_loss/len(trainloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'epoch')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(vec_loss[:], label = 'loss')\n",
    "plt.legend()\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
